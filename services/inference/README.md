# Inference Service

This service provides model inference capabilities for the Ghost Engine kernel. It exposes APIs to perform AI/ML inference, handles batching and latency optimization, and integrates with GPU accelerators when available.
